{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import dirname, join as pjoin\n",
    "import scipy.io as sio\n",
    "import h5py\n",
    "import hdf5storage\n",
    "import random\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "from matplotlib.image import imread\n",
    "import os\n",
    "from tensorflow.keras import applications\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maxim/.keras/datasets/aclImdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/maxim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_root_orig = tf.keras.utils.get_file(origin=DATASET_URL, fname='aclImdb', untar=True)\n",
    "data_root = pathlib.Path(data_root_orig)\n",
    "print(data_root)\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TRAIN = str(data_root) + '/train'\n",
    "PATH_TO_TEST =str(data_root) + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "NUM_TOKEN = '<num>'\n",
    "UNKNOWN_TOKEN = '<unk>'\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def review_to_indices(text, vocab):\n",
    "    indecies = []\n",
    "    for text_word in text:\n",
    "        word = text_word.lower()\n",
    "        index = -1\n",
    "        # try find word in vocab\n",
    "        try:\n",
    "            index = vocab.index(word)\n",
    "        except:\n",
    "            # if we can't find it - try to parse it to int\n",
    "            try:\n",
    "                i = int(word)\n",
    "                index = vocab.index(NUM_TOKEN)\n",
    "            except:\n",
    "                # else it unknown\n",
    "                index = vocab.index(UNKNOWN_TOKEN)\n",
    "        indecies.append(index)\n",
    "    return indecies\n",
    "                \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added <NUM> and <UNK> tokens\n",
      "Vocab len = 89529\n",
      "25000\n",
      "([9, 5, 2, 49, 214, 70, 621, 1527, 15, 82, 6, 5, 2, 931, 1127, 0, 356, 1182, 22, 321, 1, 0, 109, 5, 255, 6, 39, 19, 178, 48, 252, 6515, 9118, 6, 528, 79, 7, 5495, 59, 1469, 6, 388, 2, 526, 3, 2, 145, 1425, 418, 1, 19, 2, 1527, 15, 16, 3, 254, 6, 5, 2, 1527, 1, 2, 62, 48, 26, 14, 314, 1281, 461, 809, 1, 1357, 1, 30, 0, 93, 0, 381, 5, 902, 103, 44, 13, 9, 26], 8)\n",
      "25000\n",
      "([2, 175, 621, 4300, 89528, 989, 363, 739, 374, 3, 1761, 1, 562, 3, 2045, 47, 1468, 151, 1605, 2693, 743, 3, 2914, 1825, 1, 75, 1342, 353, 18, 2, 2482, 2559, 9, 43, 156, 4, 81, 14, 17075, 29, 216, 19, 7, 54, 1123, 5360, 301, 9, 1123, 278, 763, 5, 477, 38215, 37593, 1, 43, 5847, 41659, 981, 419, 6376, 4817, 1, 31241, 82, 63, 1334, 538, 23, 26, 21, 528, 0, 60, 4835, 91, 20, 24, 6376, 4817, 1, 31241, 981, 245, 78, 82, 33, 32151, 1, 0, 1334, 3309, 1, 20, 24, 31241, 981, 419, 286, 82, 21, 528, 40, 0, 1082, 1, 87, 4495, 19282, 535, 1190, 10, 528, 6, 55, 49, 11494, 7, 0, 739, 20773, 2, 386, 212, 30, 14463, 7711, 12, 10414, 11544, 5, 1676, 10, 528, 133, 8, 1201, 9, 278, 7, 26, 132, 21, 988, 4, 3077, 105, 331, 981, 30, 3505, 92, 1, 1865, 8, 292, 114, 1, 7, 153, 132, 21, 2834, 2, 3436, 7, 2790, 56036, 1, 258, 4788, 13, 2, 330, 56915, 6, 528, 79, 1676, 10, 858, 1328, 29783, 2464, 33734, 77640, 117, 0, 605, 8, 114, 9, 1220, 3, 4709, 2207, 24592, 35, 6, 11, 31, 64554, 13, 68], 3)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "# load vocab\n",
    "\n",
    "vocab = []\n",
    "\n",
    "with open(str(data_root) + '/imdb.vocab', 'r') as file:\n",
    "    vocab = file.read().splitlines()\n",
    "    vocab.append(NUM_TOKEN)\n",
    "    vocab.append(UNKNOWN_TOKEN)\n",
    "    print('Added <NUM> and <UNK> tokens')\n",
    "    print('Vocab len =', len(vocab))\n",
    "    \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "total_review_len = 0\n",
    "total_review_count = 0\n",
    "\n",
    "reviews = []\n",
    "\n",
    "for review_name in os.listdir(PATH_TO_TRAIN + '/pos'):\n",
    "    mark = int(review_name.split('_')[1].split('.')[0])\n",
    "    path = PATH_TO_TRAIN + '/pos/' + review_name\n",
    "    with open(path, 'r') as file:\n",
    "        data = file.read()\n",
    "        clean_review = cleanhtml(data)\n",
    "        indecies = review_to_indices(tokenizer.tokenize(clean_review), vocab)\n",
    "        total_review_len += len(indecies)\n",
    "        total_review_count += 1\n",
    "        reviews.append((indecies, mark))\n",
    "\n",
    "        \n",
    "for review_name in os.listdir(PATH_TO_TRAIN + '/neg'):\n",
    "    mark = int(review_name.split('_')[1].split('.')[0])\n",
    "    path = PATH_TO_TRAIN + '/neg/' + review_name\n",
    "    with open(path, 'r') as file:\n",
    "        data = file.read()\n",
    "        clean_review = cleanhtml(data)\n",
    "        indecies = review_to_indices(tokenizer.tokenize(clean_review), vocab)\n",
    "        total_review_len += len(indecies)\n",
    "        total_review_count += 1\n",
    "        reviews.append((indecies, mark))\n",
    "        \n",
    "\n",
    "print(len(reviews))\n",
    "print(reviews[10])\n",
    "\n",
    "print(len(reviews))\n",
    "print(reviews[13000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5954856\n",
      "25000\n",
      "238\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "average_review_len = int(total_review_len / total_review_count)\n",
    "\n",
    "print(total_review_len)\n",
    "print(total_review_count)\n",
    "print(average_review_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 15018, 5, 26, 3, 26776, 9238, 528, 87, 13444, 1112, 16, 7, 9, 1121, 1203, 0, 1188, 40, 17353, 854, 37, 9, 301, 1230, 178, 2703, 4, 9238, 528, 193, 61, 35, 26, 66, 854, 4535, 0, 1767, 18, 290, 40498, 528, 221, 42, 224, 0, 2074, 1771, 14, 0, 20387, 721, 0, 12514, 1023, 1, 0, 1178, 15052, 575, 13588, 26, 58, 25, 201, 4, 514, 9, 61, 4, 94, 13, 2, 653, 5605, 242, 1109, 15, 294, 0, 17, 5, 2, 5333, 734, 1534, 10, 444, 175, 2256, 4, 103, 408, 32, 22, 19, 1066, 14, 0, 61, 139, 387, 318, 0, 263, 84, 28, 0, 17, 76, 2647, 5, 4, 265, 92, 129, 318, 45, 5, 159, 2, 408, 528, 343, 179, 7663, 6128, 575, 3, 4145, 3580, 442, 7, 2, 582, 96, 2797, 9499, 478, 0, 2646, 19695, 3, 1996, 39406, 26, 2066, 51, 5, 22356, 140, 0, 1990, 50, 51, 3151, 2, 2482, 800, 1253, 31517, 183, 0, 2019, 876, 0, 800, 7433, 6, 43, 73, 107, 35, 6, 4127, 7, 140, 0, 1990, 1, 33507, 6128, 34, 38, 1411, 8903, 38, 80, 89, 4100, 6700, 155, 8737, 80, 0, 304, 6128, 506, 2078, 10, 51, 43, 73, 3410, 30, 2, 1424, 34, 1424, 661, 1, 3447, 10, 21, 76, 1858, 38, 16, 4, 38, 2089, 21, 486, 44, 4, 25, 2, 235, 1, 2715, 1611, 3, 23, 4489, 32, 4212, 300]\n",
      "[ 4  7  8 ...  2 10  9]\n",
      "[ 4942 27173     5    31   164   876  4099    32     5   623    12     2\n",
      "   596     3  5141    13    23 11191     3 29799   632   511 19591     5\n",
      "     2   781  1461  4099     3 29799   511     1  6679  4942    12    31\n",
      "  1614     1    12     2   389 19591     5  3712    10     2  3610   511\n",
      " 48917   854   820    14  4942     1    35    21 81844     0  4200     3\n",
      "    31  9058     7     4  4942   528   334     0    17     5    41  8804\n",
      "   910 10940  1091     3   511  1182     1    87  3426  4724     8    66\n",
      "   854   366     9    17   189     0   217    17     5     7 27050   272\n",
      "    43     2  4999    89     2   203    61    59    43    73  2077    35\n",
      "    71    10     0  1928     3     0    17     5    40 89527   225     2\n",
      "   918  7081    13   704   225     6     5    31  1562    17   361   704\n",
      "   225    89     2   601    17   361   704   225    89     2   695     6\n",
      "    40   903  2637    89  1265    36    89  2109  1395    23   391   226\n",
      "   132    18     0  5835     5  6651    61     1   221     5  3485   151\n",
      "    22   509   190     0  3108    22  2664    20    66   358     0    93\n",
      "    33    24  2507    52  1120     6    11    19    53  2363    13  3084\n",
      "    10   317   640  2945   182     0   115    17   787     1     7     0\n",
      "  2116    17  2330 38682  1149 38682    11    40   583     1    41   640\n",
      "  2945     0  3002   292     0   123   103     6 89527 89527]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "np_vocab = np.asarray(vocab)\n",
    "\n",
    "random.shuffle(reviews)\n",
    "\n",
    "print(reviews[0][0][:average_review_len])\n",
    "\n",
    "for review in reviews:\n",
    "    review_rep = np.asarray(review[0])\n",
    "#     one_hot_rep = np.zeros((vocab.size, review_rep.max() + 1))\n",
    "#     one_hot_rep[np.arange(review_rep.size), review_rep] = 1\n",
    "    y.append(review[1])\n",
    "    \n",
    "y = np.asarray(y)\n",
    "print(y)\n",
    "# print(review_rep)\n",
    "# one_hot_rep = np.zeros((np_vocab.size, 1))\n",
    "# one_hot_rep[review_rep[0], 0] = 1.\n",
    "# one_hot_rep[0, 0]\n",
    "# len(one_hot_rep)\n",
    "# one_hot_rep[np.arange(review_rep.size), review_rep] = 1\n",
    "# print(review_rep[0])\n",
    "# print(one_hot_rep[0][review_rep[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'0_10.txt'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
