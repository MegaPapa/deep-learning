{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import dirname, join as pjoin\n",
    "import scipy.io as sio\n",
    "import h5py\n",
    "import hdf5storage\n",
    "import random\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "from matplotlib.image import imread\n",
    "import os\n",
    "from tensorflow.keras import applications\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maxim/.keras/datasets/aclImdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/maxim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_root_orig = tf.keras.utils.get_file(origin=DATASET_URL, fname='aclImdb', untar=True)\n",
    "data_root = pathlib.Path(data_root_orig)\n",
    "print(data_root)\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TRAIN = str(data_root) + '/train'\n",
    "PATH_TO_TEST =str(data_root) + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "NUM_TOKEN = '<num>'\n",
    "UNKNOWN_TOKEN = '<unk>'\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def review_to_indices(text, vocab):\n",
    "    indecies = []\n",
    "    for text_word in text:\n",
    "        word = text_word.lower()\n",
    "        index = -1\n",
    "        # try find word in vocab\n",
    "        try:\n",
    "            index = vocab.index(word)\n",
    "        except:\n",
    "            # if we can't find it - try to parse it to int\n",
    "            try:\n",
    "                i = int(word)\n",
    "                index = vocab.index(NUM_TOKEN)\n",
    "            except:\n",
    "                # else it unknown\n",
    "                index = vocab.index(UNKNOWN_TOKEN)\n",
    "        indecies.append(index)\n",
    "    return indecies\n",
    "                \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added <NUM> and <UNK> tokens\n",
      "Vocab len = 89529\n",
      "12500\n",
      "([9, 5, 2, 49, 214, 70, 621, 1527, 15, 82, 6, 5, 2, 931, 1127, 0, 356, 1182, 22, 321, 1, 0, 109, 5, 255, 6, 39, 19, 178, 48, 252, 6515, 9118, 6, 528, 79, 7, 5495, 59, 1469, 6, 388, 2, 526, 3, 2, 145, 1425, 418, 1, 19, 2, 1527, 15, 16, 3, 254, 6, 5, 2, 1527, 1, 2, 62, 48, 26, 14, 314, 1281, 461, 809, 1, 1357, 1, 30, 0, 93, 0, 381, 5, 902, 103, 44, 13, 9, 26], 8)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "# load vocab\n",
    "\n",
    "vocab = []\n",
    "\n",
    "with open(str(data_root) + '/imdb.vocab', 'r') as file:\n",
    "    vocab = file.read().splitlines()\n",
    "    vocab.append(NUM_TOKEN)\n",
    "    vocab.append(UNKNOWN_TOKEN)\n",
    "    print('Added <NUM> and <UNK> tokens')\n",
    "    print('Vocab len =', len(vocab))\n",
    "    \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "total_review_len = 0\n",
    "total_review_count = 0\n",
    "\n",
    "reviews = []\n",
    "\n",
    "for review_name in os.listdir(PATH_TO_TRAIN + '/pos'):\n",
    "    mark = int(review_name.split('_')[1].split('.')[0])\n",
    "    path = PATH_TO_TRAIN + '/pos/' + review_name\n",
    "    with open(path, 'r') as file:\n",
    "        data = file.read()\n",
    "        clean_review = cleanhtml(data)\n",
    "        indecies = review_to_indices(tokenizer.tokenize(clean_review), vocab)\n",
    "        total_review_len += len(indecies)\n",
    "        total_review_count += 1\n",
    "        reviews.append((indecies, mark))\n",
    "\n",
    "print(len(reviews))\n",
    "print(reviews[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review_name in os.listdir(PATH_TO_TRAIN + '/pos'):\n",
    "    with open(path, 'r') as file:\n",
    "        data = file.read()\n",
    "        clean_review = cleanhtml(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'0_10.txt'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
