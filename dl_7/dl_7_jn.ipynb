{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import dirname, join as pjoin\n",
    "import scipy.io as sio\n",
    "import h5py\n",
    "import hdf5storage\n",
    "import random\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "from matplotlib.image import imread\n",
    "import os\n",
    "from tensorflow.keras import applications\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maxim/.keras/datasets/aclImdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/maxim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_root_orig = tf.keras.utils.get_file(origin=DATASET_URL, fname='aclImdb', untar=True)\n",
    "data_root = pathlib.Path(data_root_orig)\n",
    "print(data_root)\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TRAIN = str(data_root) + '/train'\n",
    "PATH_TO_TEST =str(data_root) + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "NUM_TOKEN = '<num>'\n",
    "UNKNOWN_TOKEN = '<unk>'\n",
    "EMPTY_TOKEN = '<emp>'\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def review_to_indices(text, vocab):\n",
    "    indecies = []\n",
    "    for text_word in text:\n",
    "        word = text_word.lower()\n",
    "        index = -1\n",
    "        # try find word in vocab\n",
    "        try:\n",
    "            index = vocab.index(word)\n",
    "        except:\n",
    "            # if we can't find it - try to parse it to int\n",
    "            try:\n",
    "                i = int(word)\n",
    "                index = vocab.index(NUM_TOKEN)\n",
    "            except:\n",
    "                # else it unknown\n",
    "                index = vocab.index(UNKNOWN_TOKEN)\n",
    "        indecies.append(index)\n",
    "    return indecies\n",
    "                \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added <EMP>, <NUM> and <UNK> tokens\n",
      "Vocab len = 89530\n",
      "25000\n",
      "([9, 5, 2, 49, 214, 70, 621, 1527, 15, 82, 6, 5, 2, 931, 1127, 0, 356, 1182, 22, 321, 1, 0, 109, 5, 255, 6, 39, 19, 178, 48, 252, 6515, 9118, 6, 528, 79, 7, 5495, 59, 1469, 6, 388, 2, 526, 3, 2, 145, 1425, 418, 1, 19, 2, 1527, 15, 16, 3, 254, 6, 5, 2, 1527, 1, 2, 62, 48, 26, 14, 314, 1281, 461, 809, 1, 1357, 1, 30, 0, 93, 0, 381, 5, 902, 103, 44, 13, 9, 26], 8)\n",
      "25000\n",
      "([2, 175, 621, 4300, 89529, 989, 363, 739, 374, 3, 1761, 1, 562, 3, 2045, 47, 1468, 151, 1605, 2693, 743, 3, 2914, 1825, 1, 75, 1342, 353, 18, 2, 2482, 2559, 9, 43, 156, 4, 81, 14, 17075, 29, 216, 19, 7, 54, 1123, 5360, 301, 9, 1123, 278, 763, 5, 477, 38215, 37593, 1, 43, 5847, 41659, 981, 419, 6376, 4817, 1, 31241, 82, 63, 1334, 538, 23, 26, 21, 528, 0, 60, 4835, 91, 20, 24, 6376, 4817, 1, 31241, 981, 245, 78, 82, 33, 32151, 1, 0, 1334, 3309, 1, 20, 24, 31241, 981, 419, 286, 82, 21, 528, 40, 0, 1082, 1, 87, 4495, 19282, 535, 1190, 10, 528, 6, 55, 49, 11494, 7, 0, 739, 20773, 2, 386, 212, 30, 14463, 7711, 12, 10414, 11544, 5, 1676, 10, 528, 133, 8, 1201, 9, 278, 7, 26, 132, 21, 988, 4, 3077, 105, 331, 981, 30, 3505, 92, 1, 1865, 8, 292, 114, 1, 7, 153, 132, 21, 2834, 2, 3436, 7, 2790, 56036, 1, 258, 4788, 13, 2, 330, 56915, 6, 528, 79, 1676, 10, 858, 1328, 29783, 2464, 33734, 77640, 117, 0, 605, 8, 114, 9, 1220, 3, 4709, 2207, 24592, 35, 6, 11, 31, 64554, 13, 68], 3)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "# load vocab\n",
    "\n",
    "vocab = []\n",
    "\n",
    "with open(str(data_root) + '/imdb.vocab', 'r') as file:\n",
    "    vocab = file.read().splitlines()\n",
    "    vocab.append(EMPTY_TOKEN)\n",
    "    vocab.append(NUM_TOKEN)\n",
    "    vocab.append(UNKNOWN_TOKEN)\n",
    "    print('Added <EMP>, <NUM> and <UNK> tokens')\n",
    "    print('Vocab len =', len(vocab))\n",
    "    \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "total_review_len = 0\n",
    "total_review_count = 0\n",
    "\n",
    "reviews = []\n",
    "\n",
    "for review_name in os.listdir(PATH_TO_TRAIN + '/pos'):\n",
    "    mark = int(review_name.split('_')[1].split('.')[0])\n",
    "    path = PATH_TO_TRAIN + '/pos/' + review_name\n",
    "    with open(path, 'r') as file:\n",
    "        data = file.read()\n",
    "        clean_review = cleanhtml(data)\n",
    "        indecies = review_to_indices(tokenizer.tokenize(clean_review), vocab)\n",
    "        total_review_len += len(indecies)\n",
    "        total_review_count += 1\n",
    "        reviews.append((indecies, mark))\n",
    "\n",
    "        \n",
    "for review_name in os.listdir(PATH_TO_TRAIN + '/neg'):\n",
    "    mark = int(review_name.split('_')[1].split('.')[0])\n",
    "    path = PATH_TO_TRAIN + '/neg/' + review_name\n",
    "    with open(path, 'r') as file:\n",
    "        data = file.read()\n",
    "        clean_review = cleanhtml(data)\n",
    "        indecies = review_to_indices(tokenizer.tokenize(clean_review), vocab)\n",
    "        total_review_len += len(indecies)\n",
    "        total_review_count += 1\n",
    "        reviews.append((indecies, mark))\n",
    "        \n",
    "\n",
    "print(len(reviews))\n",
    "print(reviews[10])\n",
    "\n",
    "print(len(reviews))\n",
    "print(reviews[13000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5954856\n",
      "25000\n",
      "238\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "average_review_len = int(total_review_len / total_review_count)\n",
    "\n",
    "print(total_review_len)\n",
    "print(total_review_count)\n",
    "print(average_review_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  4 ... 10 10  9]\n",
      "238\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "np_vocab = np.asarray(vocab)\n",
    "\n",
    "random.shuffle(reviews)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for review in reviews:\n",
    "    review_rep = np.asarray(review[0][:average_review_len])\n",
    "    x.append(review_rep)\n",
    "#     one_hot_rep = np.zeros((vocab.size, review_rep.max() + 1))\n",
    "#     one_hot_rep[np.arange(review_rep.size), review_rep] = 1\n",
    "    y.append(review[1])\n",
    "    \n",
    "y = np.asarray(y)\n",
    "x = sequence.pad_sequences(x, maxlen=average_review_len)\n",
    "\n",
    "# print(review_rep)\n",
    "# one_hot_rep = np.zeros((np_vocab.size, 1))\n",
    "# one_hot_rep[review_rep[0], 0] = 1.\n",
    "# one_hot_rep[0, 0]\n",
    "# len(one_hot_rep)\n",
    "# one_hot_rep[np.arange(review_rep.size), review_rep] = 1\n",
    "# print(review_rep[0])\n",
    "# print(one_hot_rep[0][review_rep[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 238, 32)           2864960   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 2,919,170\n",
      "Trainable params: 2,919,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMB_SIZE=32\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "\n",
    "emb_1 = Embedding(len(vocab), EMB_SIZE, input_length=average_review_len)\n",
    "lstm_1 = LSTM(100)\n",
    "out = Dense(10, activation='softmax')\n",
    "\n",
    "model=Sequential([\n",
    "    emb_1,\n",
    "    lstm_1,\n",
    "    out\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_oh = np.zeros((y.size, y.max()+1))\n",
    "y_oh[np.arange(y.size),y] = 1\n",
    "y_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_6 to have shape (10,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-e0618a37c901>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/deep-learning-wM6lnET7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/deep-learning-wM6lnET7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/deep-learning-wM6lnET7/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_6 to have shape (10,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
