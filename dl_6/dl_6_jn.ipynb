{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import dirname, join as pjoin\n",
    "import scipy.io as sio\n",
    "import h5py\n",
    "import hdf5storage\n",
    "import random\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "from matplotlib.image import imread\n",
    "import os\n",
    "from tensorflow.keras import applications\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим заранее подготовленный датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (27455, 785)\n",
      "Test set: (7172, 785)\n",
      "(27455, 24)\n",
      "(7172, 24)\n"
     ]
    }
   ],
   "source": [
    "DATASET_ROOT = '/home/maxim/Desktop/ml_dataset/sign-language-mnist/'\n",
    "\n",
    "train_data = pd.read_csv(DATASET_ROOT + '/sign_mnist_train.csv')\n",
    "test_data = pd.read_csv(DATASET_ROOT + '/sign_mnist_test.csv')\n",
    "print('Train set: ' + str(train_data.shape))\n",
    "print('Test set: ' + str(test_data.shape))\n",
    "\n",
    "binarizer = LabelBinarizer()\n",
    "\n",
    "train_labels = train_data['label'].values\n",
    "train_labels = binarizer.fit_transform(train_labels)\n",
    "train_data.drop('label', axis = 1, inplace = True)\n",
    "\n",
    "test_labels = test_data['label'].values\n",
    "test_labels = binarizer.fit_transform(test_labels)\n",
    "test_data.drop('label', axis = 1, inplace = True)\n",
    "\n",
    "print(str(train_labels.shape))\n",
    "print(str(test_labels.shape))\n",
    "\n",
    "out_count = test_labels.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 784)\n",
      "(7172, 784)\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SET_RATE = 0.05\n",
    "\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "IMAGE_CHANNELS = 1\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_COUNT = 5\n",
    "\n",
    "train_values = train_data.values\n",
    "test_values = test_data.values\n",
    "print(train_values.shape)\n",
    "print(test_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьём датасет на выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26082, 28, 28, 1)\n",
      "(1373, 28, 28, 1)\n",
      "(7172, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# SPLIT ON SETS\n",
    "x_train = np.asarray(train_values).reshape(train_values.shape[0], IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)\n",
    "x_test = np.asarray(test_values).reshape(test_values.shape[0], IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(\n",
    "    x_train,\n",
    "    train_labels,\n",
    "    test_size = VALIDATION_SET_RATE, \n",
    "    stratify = train_labels\n",
    ")\n",
    "y_test = test_labels\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем данные при помощи ImageDataGenerator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26082, 32, 32, 3)\n",
      "(7172, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# PUT IN IMAGE DATA GENERATOR\n",
    "\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "test_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "validation_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "au_validation_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "au_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    width_shift_range=0.2, \n",
    "    height_shift_range=0.2, \n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "data_loader = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "modified_image_shape = (32, 32, 3)\n",
    "\n",
    "x_train_vgg = np.pad(\n",
    "    x_train,\n",
    "    ((0,0),(2,2),(2,2),(0,0)),\n",
    "    'edge'\n",
    ")\n",
    "x_train_vgg = np.repeat(x_train_vgg, 3, axis=3)\n",
    "train_flow_vgg = data_loader.flow(\n",
    "    x_train_vgg,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "x_test_vgg = np.pad(\n",
    "    x_test,\n",
    "    ((0,0),(2,2),(2,2),(0,0)),\n",
    "    'edge'\n",
    ")\n",
    "x_test_vgg = np.repeat(x_test_vgg, 3, axis=3)\n",
    "test_flow_vgg = data_loader.flow(\n",
    "    x_test_vgg,\n",
    "    y_test,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "print(x_train_vgg.shape)\n",
    "print(x_test_vgg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCRIBE FLOWS\n",
    "\n",
    "train_flow = image_generator.flow(x_train, y_train)\n",
    "validation_flow = validation_image_generator.flow(x_validation, y_validation)\n",
    "test_flow = test_image_generator.flow(x_test, y_test)\n",
    "au_train_flow = au_image_generator.flow(x_train, y_train)\n",
    "au_validation_flow = au_validation_image_generator.flow(x_validation, y_validation)\n",
    "# vgg_flow = vgg_image_generator.flow(x_train, y_train)\n",
    "# resnet_flow = resnet_image_generator.flow(x_train, y_train)\n",
    "\n",
    "STEPS_PER_EPOCH_TRAIN = 600\n",
    "AU_STEPS_PER_EPOCH_TRAIN = 600\n",
    "VGG_STEPS_PER_EPOCH_TRAIN = 600\n",
    "RESNET_STEPS_PER_EPOCH_TRAIN = 800\n",
    "STEPS_PER_EPOCH_VALIDATION = 10\n",
    "AU_STEPS_PER_EPOCH_VALIDATION = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим первую модель, данная модель состоит из 3х пар свёртка-пулинг и 3х полносвязных слоёв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                1560      \n",
      "=================================================================\n",
      "Total params: 41,432\n",
      "Trainable params: 41,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# DEFINE THE MODEL\n",
    "\n",
    "input_layer = layers.Input(shape=((IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)))\n",
    "conv_1 = layers.Conv2D(16, kernel_size=(3, 3), padding='valid')\n",
    "conv_2 = layers.Conv2D(32, kernel_size=(3, 3), padding='valid')\n",
    "conv_3 = layers.Conv2D(64, kernel_size=(3, 3), padding='valid')\n",
    "mp_1 = layers.MaxPooling2D(strides=(2, 2))\n",
    "mp_2 = layers.MaxPooling2D(strides=(2, 2))\n",
    "mp_3 = layers.MaxPooling2D(strides=(2, 2))\n",
    "\n",
    "fc_1 = layers.Dense(128, activation='relu')\n",
    "fc_2 = layers.Dense(64, activation='relu')\n",
    "\n",
    "out_layer = layers.Dense(out_count, activation='sigmoid')\n",
    "\n",
    "model = keras.Sequential([\n",
    "        input_layer,\n",
    "        conv_1,\n",
    "        mp_1,\n",
    "        conv_2,\n",
    "        mp_2,\n",
    "        conv_3,\n",
    "        mp_3,\n",
    "        layers.Flatten(),\n",
    "        fc_1,\n",
    "        fc_2,\n",
    "        out_layer\n",
    "    ])\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для упрощения отладки - модели будем сохранять:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-fc13a9309981>:12: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 600 steps, validate for 10 steps\n",
      "Epoch 1/5\n",
      "600/600 [==============================] - 4s 6ms/step - loss: 1.6572 - accuracy: 0.4653 - val_loss: 0.5905 - val_accuracy: 0.8250\n",
      "Epoch 2/5\n",
      "600/600 [==============================] - 6s 9ms/step - loss: 0.3178 - accuracy: 0.8953 - val_loss: 0.1533 - val_accuracy: 0.9594\n",
      "Epoch 3/5\n",
      "600/600 [==============================] - 6s 10ms/step - loss: 0.0778 - accuracy: 0.9768 - val_loss: 0.0592 - val_accuracy: 0.9812\n",
      "Epoch 4/5\n",
      "600/600 [==============================] - 6s 10ms/step - loss: 0.0370 - accuracy: 0.9898 - val_loss: 0.0159 - val_accuracy: 0.9937\n",
      "Epoch 5/5\n",
      "600/600 [==============================] - 6s 10ms/step - loss: 0.0271 - accuracy: 0.9927 - val_loss: 0.0044 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "if os.path.exists('three_conv.h5'):\n",
    "    model = load_model('three_conv.h5')\n",
    "else:\n",
    "    history = model.fit_generator(\n",
    "        train_flow,\n",
    "        validation_data=validation_flow,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH_TRAIN,\n",
    "        validation_steps=STEPS_PER_EPOCH_VALIDATION,\n",
    "        epochs=EPOCHS_COUNT,\n",
    "        verbose=1\n",
    "    )\n",
    "    model.save('three_conv.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.8680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.519660881674952, 0.8679587]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данная модель показывает неплохие данные данные - 86% точности на тестовой выборке.\n",
    "\n",
    "Далее определим схожу с предыдущей модель, однако при её обучении будем использовать augumented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                1560      \n",
      "=================================================================\n",
      "Total params: 41,432\n",
      "Trainable params: 41,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "au_input_layer = layers.Input(shape=((IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)))\n",
    "au_conv_1 = layers.Conv2D(16, kernel_size=(3, 3), padding='valid')\n",
    "au_conv_2 = layers.Conv2D(32, kernel_size=(3, 3), padding='valid')\n",
    "au_conv_3 = layers.Conv2D(64, kernel_size=(3, 3), padding='valid')\n",
    "au_mp_1 = layers.MaxPooling2D(strides=(2, 2))\n",
    "au_mp_2 = layers.MaxPooling2D(strides=(2, 2))\n",
    "au_mp_3 = layers.MaxPooling2D(strides=(2, 2))\n",
    "\n",
    "au_fc_1 = layers.Dense(128, activation='relu')\n",
    "au_fc_2 = layers.Dense(64, activation='relu')\n",
    "\n",
    "au_out_layer = layers.Dense(out_count, activation='sigmoid')\n",
    "\n",
    "au_model = keras.Sequential([\n",
    "        au_input_layer,\n",
    "        au_conv_1,\n",
    "        au_mp_1,\n",
    "        au_conv_2,\n",
    "        au_mp_2,\n",
    "        au_conv_3,\n",
    "        au_mp_3,\n",
    "        layers.Flatten(),\n",
    "        au_fc_1,\n",
    "        au_fc_2,\n",
    "        au_out_layer\n",
    "    ])\n",
    "au_model.summary()\n",
    "\n",
    "au_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для упрощения отладки, будем сохранять модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 600 steps, validate for 10 steps\n",
      "Epoch 1/5\n",
      "600/600 [==============================] - 9s 15ms/step - loss: 2.8143 - accuracy: 0.1341 - val_loss: 1.8625 - val_accuracy: 0.3656\n",
      "Epoch 2/5\n",
      "600/600 [==============================] - 8s 14ms/step - loss: 1.7719 - accuracy: 0.4107 - val_loss: 1.1812 - val_accuracy: 0.6094\n",
      "Epoch 3/5\n",
      "600/600 [==============================] - 9s 14ms/step - loss: 1.2785 - accuracy: 0.5640 - val_loss: 0.8722 - val_accuracy: 0.7125\n",
      "Epoch 4/5\n",
      "600/600 [==============================] - 8s 14ms/step - loss: 1.0612 - accuracy: 0.6378 - val_loss: 0.7069 - val_accuracy: 0.7594\n",
      "Epoch 5/5\n",
      "600/600 [==============================] - 8s 14ms/step - loss: 0.9116 - accuracy: 0.6908 - val_loss: 0.6182 - val_accuracy: 0.7875\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('three_conv_au.h5'):\n",
    "    au_model = load_model('three_conv_au.h5')\n",
    "else:\n",
    "    au_history = au_model.fit_generator(\n",
    "        au_train_flow,\n",
    "        validation_data=au_validation_flow,\n",
    "        steps_per_epoch=AU_STEPS_PER_EPOCH_TRAIN,\n",
    "        validation_steps=AU_STEPS_PER_EPOCH_VALIDATION,\n",
    "        epochs=EPOCHS_COUNT,\n",
    "        verbose=1\n",
    "    )\n",
    "    au_model.save('three_conv_au.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 0.8112 - accuracy: 0.6924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.811235894229677, 0.69241494]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On augumented 3-conv\n",
    "au_model.evaluate(test_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, данная модель показывает себя чуть хуже в процентном соотношении, однако стоит отметить, что она обучается медленнее и, вероятно, при использовании большего количества эпох результат модели будет лучше. Однако для чистоты сравнения - будем использовать одинаковое количество эпох.\n",
    "\n",
    "Следующей моделью будет VGG-16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                3096      \n",
      "=================================================================\n",
      "Total params: 14,783,448\n",
      "Trainable params: 68,760\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# VGG-16\n",
    "keras.backend.clear_session()\n",
    "    \n",
    "# Load model and mark pretrained layers as not trainable\n",
    "vgg16 = applications.VGG16(include_top=False, input_shape=(32, 32, 3))\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "        \n",
    "# Forward pretrained model output into new layers\n",
    "x = layers.Flatten()(vgg16.layers[-1].output)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "output = layers.Dense(out_count, activation='sigmoid')(x)\n",
    "    \n",
    "# Compile model\n",
    "vgg16 = keras.Model(inputs=vgg16.inputs, outputs=output)\n",
    "vgg16.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 600 steps\n",
      "Epoch 1/3\n",
      "600/600 [==============================] - 106s 176ms/step - loss: 1.1921 - accuracy: 0.6651\n",
      "Epoch 2/3\n",
      "600/600 [==============================] - 109s 182ms/step - loss: 0.4174 - accuracy: 0.8937\n",
      "Epoch 3/3\n",
      "600/600 [==============================] - 106s 176ms/step - loss: 0.2405 - accuracy: 0.9479\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('vgg16.h5'):\n",
    "    vgg16 = load_model('vgg16.h5')\n",
    "else:\n",
    "    vgg_history = vgg16.fit_generator(\n",
    "        train_flow_vgg,\n",
    "        steps_per_epoch=VGG_STEPS_PER_EPOCH_TRAIN,\n",
    "        epochs=3,\n",
    "        verbose=1\n",
    "    )\n",
    "    vgg16.save('vgg16.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из процесса обучения - данная модель обучается медленнее остальных, однако стоит отметить, что она выдаёт хорошую точность на обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "225/225 [==============================] - 40s 178ms/step - loss: 0.4044 - accuracy: 0.8610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.40436958938837053, 0.8609872]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.evaluate(test_flow_vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На тестовых данных данная модель показала себя так же неплохо.\n",
    "\n",
    "Можно отметить, что все модели показали себя достаточно хорошо, выдавая точность выше 80%. Где худший результат был у модели использующей аугументированные данных, возможно, из за специфики распознавания, где не особо допускаются координальные изменения изображений."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
